{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T16:08:49.476084Z",
     "start_time": "2019-01-12T16:08:48.240843Z"
    }
   },
   "outputs": [],
   "source": [
    "#设置每行输出\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" \n",
    "#下载数据集\n",
    "from sklearn import datasets\n",
    "iris=datasets.load_iris()\n",
    "list(iris.keys())\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T16:08:49.486072Z",
     "start_time": "2019-01-12T16:08:49.479077Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(2042)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T16:08:49.528046Z",
     "start_time": "2019-01-12T16:08:49.496067Z"
    }
   },
   "outputs": [],
   "source": [
    "def logprint(key,msg=None):\n",
    "    type_key = type(key)\n",
    "    if type_key ==str:\n",
    "        shape = len(key)\n",
    "    elif type_key ==np.ndarray:\n",
    "        shape =key.shape\n",
    "    else:\n",
    "        shape =None\n",
    "    if shape !=None:\n",
    "        print((\"logprint\\tmsg:\"+str(msg)+\"\\ttype:\"+str(type(key))+\"\\tshape:\"+str(shape)))\n",
    "    else:\n",
    "        print((\"logprint\\tmsg:\"+str(msg)+\"\\ttype:\"+str(type(key))))\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T16:08:49.693947Z",
     "start_time": "2019-01-12T16:08:49.531045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90, 3), (30, 3), (30, 3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "((90,), (30,), (30,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(90, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(30, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(30, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#sample 样例数量\n",
    "x= iris[\"data\"][:,(2,3)]\n",
    "y=iris[\"target\"]\n",
    "x_with_bias = np.c_[np.ones([len(x), 1]), x]\n",
    "\n",
    "test_ratio = 0.2\n",
    "validation_ratio = 0.2\n",
    "total_size = len(x_with_bias)\n",
    "\n",
    "test_size = int(total_size * test_ratio)\n",
    "validation_size = int(total_size * validation_ratio)\n",
    "train_size = total_size - test_size - validation_size\n",
    "\n",
    "rnd_indices = np.random.permutation(total_size)\n",
    "\n",
    "x_train = x_with_bias[rnd_indices[:train_size]]\n",
    "y_train = y[rnd_indices[:train_size]]\n",
    "x_valid = x_with_bias[rnd_indices[train_size:-test_size]]\n",
    "y_valid = y[rnd_indices[train_size:-test_size]]\n",
    "x_test = x_with_bias[rnd_indices[-test_size:]]\n",
    "y_test = y[rnd_indices[-test_size:]]\n",
    "x_train.shape,x_valid.shape,x_test.shape\n",
    "y_train.shape,y_valid.shape,y_test.shape\n",
    "n_classes = y.max()+1\n",
    "\n",
    "#np.arange 形成一个等差的数组 \n",
    "#np.[[k_a,k_b,k_c],[v_a,v_b,v_c]] 相当于对矩阵的某些行和列幅值\n",
    "def to_one_hot(y):\n",
    "#     n_classes = y.max() + 1\n",
    "    m = len(y)\n",
    "    y_one_hot = np.zeros((m, n_classes))\n",
    "    y_one_hot[np.arange(m), y] = 1\n",
    "    return y_one_hot\n",
    "y_train_one_hot = to_one_hot(y_train)\n",
    "y_valid_one_hot = to_one_hot(y_valid)\n",
    "y_test_one_hot = to_one_hot(y_test) \n",
    "y_train_one_hot.shape\n",
    "y_valid_one_hot.shape\n",
    "y_test_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T08:13:41.756739Z",
     "start_time": "2019-01-12T08:13:41.747744Z"
    }
   },
   "source": [
    "下面实现Softmax function.\n",
    "\n",
    "回忆一下它的定义：\n",
    "\n",
    "$\\sigma\\left(\\mathbf{s}(\\mathbf{x})\\right)_k = \\dfrac{\\exp\\left(s_k(\\mathbf{x})\\right)}{\\sum\\limits_{j=1}^{K}{\\exp\\left(s_j(\\mathbf{x})\\right)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T16:08:49.710933Z",
     "start_time": "2019-01-12T16:08:49.698941Z"
    }
   },
   "outputs": [],
   "source": [
    "# 其实就是用代码实现这个数学公式\n",
    "#p[[p11,p12,...,p1k],...,[pm1,pm2,...,pmk]]\n",
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    exp_sums = np.sum(exps, axis=1, keepdims=True)\n",
    "    return exps / exp_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T16:08:49.738916Z",
     "start_time": "2019-01-12T16:08:49.714931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprint\tmsg:theta初始随机值\ttype:<class 'numpy.ndarray'>\tshape:(3, 3)\n",
      "[[ 0.5593025   1.29143558  2.27356226]\n",
      " [ 0.68283041 -0.25129965  0.62717215]\n",
      " [-0.18749445  0.71044056  1.04302869]]\n"
     ]
    }
   ],
   "source": [
    "n_features = x_train.shape[1] # == 3 (2 features plus the bias term)\n",
    "np.random.seed(2042)\n",
    "theta = np.random.randn(n_features,n_classes)\n",
    "logprint(theta,\"theta初始随机值\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T16:08:56.118976Z",
     "start_time": "2019-01-12T16:08:49.745912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.561571331332843\n",
      "500 0.9708231098717285\n",
      "1000 0.808076973013991\n",
      "1500 0.7193544146997736\n",
      "2000 0.6674970337024547\n",
      "2500 0.6349793995124956\n",
      "3000 0.6132793603963999\n",
      "3500 0.598007690058714\n",
      "4000 0.5867693362133265\n",
      "4500 0.5781874033461191\n",
      "5000 0.5714325137264352\n",
      "5500 0.5659835808488032\n",
      "6000 0.5615003825992032\n",
      "6500 0.5577528165943716\n",
      "7000 0.5545801418248097\n",
      "7500 0.5518666951805085\n",
      "8000 0.5495269730074995\n",
      "8500 0.5474962035408695\n",
      "9000 0.5457242312651907\n",
      "9500 0.5441714516840684\n",
      "10000 0.542806046449744\n",
      "10500 0.5416020618465286\n",
      "11000 0.5405380458041201\n",
      "11500 0.5395960621678977\n",
      "12000 0.5387609645838005\n",
      "12500 0.5380198522423288\n",
      "13000 0.5373616551965559\n",
      "13500 0.5367768135167515\n",
      "14000 0.5362570254705775\n",
      "14500 0.5357950472435338\n",
      "15000 0.5353845316978185\n",
      "15500 0.5350198971052539\n",
      "16000 0.5346962191929354\n",
      "16500 0.5344091415418948\n",
      "17000 0.5341548005992403\n",
      "17500 0.5339297624498602\n",
      "18000 0.533730969144296\n",
      "18500 0.5335556928626771\n",
      "19000 0.5334014965577909\n",
      "19500 0.5332661999961692\n",
      "20000 0.5331478503278173\n",
      "20500 0.5330446964793457\n",
      "21000 0.5329551667937824\n",
      "21500 0.5328778494418731\n",
      "22000 0.532811475210581\n",
      "22500 0.5327549023395481\n",
      "23000 0.5327071031289276\n",
      "23500 0.5326671520849638\n",
      "24000 0.5326342154049968\n",
      "24500 0.5326075416327348\n",
      "25000 0.5325864533389075\n",
      "25500 0.5325703397027002\n",
      "26000 0.5325586498864284\n",
      "26500 0.5325508871103141\n",
      "27000 0.5325466033464545\n",
      "27462 0.5325453868907144\n",
      "27463 0.5325453868909262 early stopping!\n"
     ]
    }
   ],
   "source": [
    "n_iterations =100001\n",
    "m_train = len(x_train)\n",
    "eta = 0.01\n",
    "epsilon = 1e-7\n",
    "\n",
    "n_classes = len(np.unique(y_train)) \n",
    "alpha=0.1\n",
    "best_loss =np.infty\n",
    "for iteration in range(n_iterations):\n",
    "    logits = x_train.dot(theta)\n",
    "    y_proba = softmax(logits)\n",
    "#     y_train_one_hot.shape\n",
    "    # k*d+1\n",
    "    gradients_t =np.zeros(theta.shape).T\n",
    "    xentropy_loss = -np.mean(np.sum(y_train_one_hot * np.log(y_proba + epsilon), axis=1))\n",
    "    l2_loss =  1/2 * np.sum(np.square(theta[1:]))\n",
    "    loss = xentropy_loss + alpha * l2_loss\n",
    "#实现一 按照公式推导做法 \n",
    "#     error =y_proba-y_train_one_hot\n",
    "#     for k  in range(n_classes):\n",
    "#         #将theta_k 的梯度填充进去\n",
    "#         gradients_t[k] = 1./m_train *error.T[k].dot(x_train)\n",
    "#     gradients = gradients_t.T + np.r_[np.zeros([1, n_classes]), alpha * theta[1:]]\n",
    "#     theta = theta -eta * gradients    \n",
    "##实现二\n",
    "    error =y_proba-y_train_one_hot\n",
    "    gradients_t=1/m_train *error.T.dot(x_train)\n",
    "    gradients = gradients_t.T +  np.r_[np.zeros([1, n_classes]), alpha * theta[1:]]\n",
    "    theta = theta -eta * gradients\n",
    "#实现三 \n",
    "#     error =y_proba-y_train_one_hot\n",
    "#     gradients = 1/m_train * x_train.T.dot(error) + np.r_[np.zeros([1, n_classes]), alpha * theta[1:]]\n",
    "#     theta = theta - eta * gradients\n",
    "##加入早停\n",
    "    logits_v = x_valid.dot(theta)\n",
    "    y_proba_v = softmax(logits_v)\n",
    "    xentropy_loss_v = -np.mean(np.sum(y_valid_one_hot * np.log(y_proba_v + epsilon), axis=1))\n",
    "    l2_loss_v = 1/2 * np.sum(np.square(theta[1:]))\n",
    "    loss = xentropy_loss_v + alpha * l2_loss_v\n",
    "\n",
    "    if iteration % 500 == 0:\n",
    "        print(iteration, loss)\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "    else:\n",
    "        print(iteration - 1, best_loss)\n",
    "        print(iteration, loss, \"early stopping!\")\n",
    "        break        \n",
    "# logprint(theta,\"theta迭代%s\"%str(n_iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T16:08:56.140965Z",
     "start_time": "2019-01-12T16:08:56.122977Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = x_valid.dot(theta)\n",
    "y_proba = softmax(logits)\n",
    "y_predict = np.argmax(y_proba, axis=1)\n",
    "\n",
    "accuracy_score = (np.mean(y_predict == y_valid))\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T16:08:56.173945Z",
     "start_time": "2019-01-12T16:08:56.150960Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#最终测试集准确度 \n",
    "logits = x_test.dot(theta)\n",
    "y_proba = softmax(logits)\n",
    "y_predict = np.argmax(y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_test)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T16:08:56.201926Z",
     "start_time": "2019-01-12T16:08:56.178942Z"
    }
   },
   "outputs": [],
   "source": [
    "# 公式写错\n",
    "# n_iterations =50\n",
    "# m_train = len(x_train)\n",
    "# eta = 0.01\n",
    "# epsilon = 1e-7\n",
    "# # n_outputs = len(np.unique(y_train)) \n",
    "# theta = np.random.rand(n_features,n_classes)\n",
    "# logprint(theta,\"theta初始随机值\")\n",
    "# for iteration in range(n_iterations):\n",
    "#     logits = x_train.dot(theta)\n",
    "#     y_proba = softmax(logits)\n",
    "# #     y_train_one_hot.shape\n",
    "#     # k*d+1\n",
    "#     gradients_t =np.zeros(theta.shape).T\n",
    "#     loss = -np.mean(np.sum(y_train_one_hot * np.log(y_proba + epsilon), axis=1))\n",
    "#     if iteration % 500 == 0:\n",
    "#         print((iteration, loss))\n",
    "# #     print(\"gradients.shape:\\t\"+str(gradients.shape))\n",
    "#     for k  in range(n_classes):\n",
    "#         np_sum =np.zeros([1,n_features])\n",
    "#         for i in range(m_train):\n",
    "#             #第I个实例\n",
    "#             np_sum +=(y_proba[i][k]-y_train_one_hot[i][k])*x_train[i]  \n",
    "        \n",
    "#         #将theta_k 的梯度填充进去\n",
    "#         gradients_t[k]=np_sum*1/m_train\n",
    "#     gradients = gradients_t.T\n",
    "#     theta = theta -eta * gradients\n",
    "# #     logprint(theta,\"theta迭代%s\"%str(iteration+1))\n",
    "# logprint(theta,\"theta迭代%s\"%str(n_iterations))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
