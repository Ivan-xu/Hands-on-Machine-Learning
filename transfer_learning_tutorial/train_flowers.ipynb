{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_flowers.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1A9n3N3_3cV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# !rm datasets/inception_v3.ckpt\n",
        "# !rm log/*\n",
        "# !rm log_eval/*\n",
        "# ! mv log logbak\n",
        "num_epochs_eval =2\n",
        "num_epochs_train =2\n",
        "!ls -lrth log_eval\n",
        "#State your log directory where you can retrieve your model\n",
        "log_dir = 'log'\n",
        "\n",
        "#Create a new evaluation log directory to visualize the validation process\n",
        "log_eval = 'log_eval'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvDvBrARe8ZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "from datetime import datetime\n",
        "date = datetime.now().strftime('%Y%m%d%H')\n",
        "log_dir = './log'\n",
        "datasets_dir = './datasets'\n",
        "def_path_log = log_dir+'/'+str(date)+'.log'\n",
        "write_size =2048\n",
        "tmp_content =''\n",
        "def mprint(*args,**kwargs):\n",
        "  global tmp_content\n",
        "  saved_args = locals()\n",
        "  time=datetime.now().strftime('%H:%M:%S')    \n",
        "#   print (*args,**kwargs)   \n",
        "  tmp_content =str(time)+':'+str(saved_args)+'\\n'+tmp_content\n",
        "  if sys.getsizeof(tmp_content)>=write_size:    \n",
        "    with open(def_path_log,\"a\") as f:\n",
        "      f.write(tmp_content)\n",
        "    tmp_content ='' \n",
        "mail_key = ['on','off'][0]\n",
        "\n",
        "my_sender='13660526535@139.com'    # 发件人邮箱账号\n",
        "my_pass = 'Bwcx7078@139'              # 发件人邮箱密码(当时申请smtp给的口令)\n",
        "my_user='13660526535@139.com'    # 发件人邮箱账号\n",
        "\n",
        "\n",
        "import smtplib\n",
        "from email.mime.text import MIMEText\n",
        "from email.utils import formataddr\n",
        "def mail(orimsg=''):\n",
        "  if mail_key =='off':\n",
        "    mprint('邮件通知关闭',orimsg)\n",
        "    return True\n",
        "  ret=True\n",
        "  try:\n",
        "    if orimsg=='':\n",
        "        msg=MIMEText('填写邮件内容','plain','utf-8')\n",
        "    else:\n",
        "        msg=MIMEText(orimsg,'plain','utf-8')\n",
        "    msg['From']=formataddr([\"我的139邮箱\",my_sender])  # 括号里的对应发件人邮箱昵称、发件人邮箱账号\n",
        "    msg['To']=formataddr([\"我的QQ邮箱\",my_user])              # 括号里的对应收件人邮箱昵称、收件人邮箱账号\n",
        "    msg['Subject']=\"卷积网络CNN\"                # 邮件的主题，也可以说是标题\n",
        "\n",
        "    server=smtplib.SMTP_SSL(\"smtp.139.com\", 465)  # 发件人邮箱中的SMTP服务器，端口是465\n",
        "    server.login(my_sender, my_pass)  # 括号中对应的是发件人邮箱账号、邮箱密码\n",
        "    server.sendmail(my_sender,[my_user,],msg.as_string())  # 括号中对应的是发件人邮箱账号、收件人邮箱账号、发送邮件\n",
        "    server.quit()# 关闭连接\n",
        "    mprint(\"邮件发送成功\",orimsg)\n",
        "  except Exception,info :# 如果 try 中的语句没有执行，则会执行下面的 ret=False\n",
        "    ret=False\n",
        "    print(\"邮件发送失败:%s\"%str(info))\n",
        "  return ret\n",
        "# mail('test mail ok')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXu8kUwEWbp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#coding=gbk\n",
        "#author:liangliang\n",
        "#email:liangliangyy@gmail.com\n",
        "#blog:http://www.lylinux.org/\n",
        "\n",
        "from ftplib import FTP\n",
        "import os,sys,string,time\n",
        "import socket\n",
        "\n",
        "        \n",
        "# def mprint (data,msg=''):\n",
        "#     time=datetime.now().strftime('%H:%M:%S')    \n",
        "#     print (msg)\n",
        "#     print (data)\n",
        "#     print('\\n')\n",
        "#     with open(def_path_log,\"a\") as f:\n",
        "#         f.write(str(time)+': '+msg+'\\n'+str(data)+'\\n\\n')   \n",
        "\n",
        "class MYFTP:\n",
        "  def __init__(self, hostaddr, username, password, remotedir, port=21):\n",
        "    self.hostaddr = hostaddr\n",
        "    self.username = username\n",
        "    self.password = password\n",
        "    self.remotedir  = remotedir\n",
        "    self.port     = port\n",
        "    self.ftp      = FTP()\n",
        "    self.file_list = []\n",
        "    # self.ftp.set_debuglevel(2)\n",
        "  def __del__(self):\n",
        "    self.ftp.close()\n",
        "    # self.ftp.set_debuglevel(0)\n",
        "  def login(self):\n",
        "    ftp = self.ftp\n",
        "    try: \n",
        "      timeout = 60\n",
        "      socket.setdefaulttimeout(timeout)\n",
        "      ftp.set_pasv(True)\n",
        "#       print '开始连接到 %s' %(self.hostaddr)\n",
        "      ftp.connect(self.hostaddr, self.port)\n",
        "#       print '成功连接到 %s' %(self.hostaddr)\n",
        "#       print '开始登录到 %s' %(self.hostaddr)\n",
        "      ftp.login(self.username, self.password)\n",
        "#       print '成功登录到 %s' %(self.hostaddr)\n",
        "      debug_print(ftp.getwelcome())\n",
        "    except Exception:\n",
        "      deal_error(\"连接或登录失败\")\n",
        "    try:\n",
        "      ftp.cwd(self.remotedir)\n",
        "    except(Exception):\n",
        "      deal_error('切换目录失败')\n",
        "\n",
        "  def is_same_size(self, localfile, remotefile):\n",
        "    try:\n",
        "      remotefile_size = self.ftp.size(remotefile)\n",
        "    except:\n",
        "      remotefile_size = -1\n",
        "    try:\n",
        "      localfile_size = os.path.getsize(localfile)\n",
        "    except:\n",
        "      localfile_size = -2\n",
        "    debug_print('lo:%d  re:%d' %(localfile_size, remotefile_size),)\n",
        "    if remotefile_size == localfile_size:\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "  def download_file(self, localfile, remotefile):\n",
        "    print (\"remotefile:\",remotefile)\n",
        "    if self.is_same_size(localfile, remotefile):\n",
        "      debug_print('%s 文件大小相同，无需下载' %localfile)\n",
        "      return\n",
        "    else:\n",
        "      debug_print('>>>>>>>>>>>>下载文件 %s ... ...' %localfile)\n",
        "    #return\n",
        "    file_handler = open(localfile, 'wb')\n",
        "    self.ftp.retrbinary('RETR %s'%(remotefile), file_handler.write)\n",
        "    file_handler.close()\n",
        "\n",
        "  def download_files(self, localdir='./', remotedir='./'):\n",
        "    try:\n",
        "      self.ftp.cwd(remotedir)\n",
        "    except:\n",
        "      debug_print('目录%s不存在，继续...' %remotedir)      \n",
        "      return\n",
        "    if not os.path.isdir(localdir):\n",
        "      os.makedirs(localdir)\n",
        "    debug_print('切换至目录 %s' %self.ftp.pwd())\n",
        "    self.file_list = []\n",
        "    self.ftp.dir(self.get_file_list)\n",
        "    remotenames = self.file_list\n",
        "    #print(remotenames)\n",
        "    #return\n",
        "    for item in remotenames:\n",
        "      filetype = item[0]\n",
        "      filename = item[1]\n",
        "      local = os.path.join(localdir, filename)\n",
        "      if filetype == 'd':\n",
        "        self.download_files(local, filename)\n",
        "      elif filetype == '-':\n",
        "        self.download_file(local, filename)\n",
        "    self.ftp.cwd('..')\n",
        "    debug_print('返回上层目录 %s' %self.ftp.pwd())\n",
        "  def upload_file(self, localfile, remotefile):\n",
        "    print (\"localfile:\",localfile)\n",
        "    if not os.path.isfile(localfile):\n",
        "      debug_print(\"本地文件不存在\")\n",
        "      return\n",
        "    if self.is_same_size(localfile, remotefile):\n",
        "      debug_print('跳过[相等]: %s' %localfile)\n",
        "      return\n",
        "    else:\n",
        "      try:\n",
        "        file_handler = open(localfile, 'rb')\n",
        "        self.ftp.storbinary('STOR %s' %remotefile, file_handler)\n",
        "        file_handler.close()\n",
        "        debug_print('已传送: %s' %localfile)\n",
        "      except:\n",
        "        debug_print('传送失败: %s' %localfile)\n",
        "  def upload_files(self, localdir='./', remotedir = './'):\n",
        "    if not os.path.isdir(localdir):\n",
        "      return\n",
        "    localnames = os.listdir(localdir)\n",
        "    localnames.sort( reverse=True)\n",
        "    print (\"localnames\",localnames)\n",
        "    self.ftp.cwd(remotedir)\n",
        "    for item in localnames:\n",
        "      src = os.path.join(localdir, item)\n",
        "      if os.path.isdir(src):\n",
        "        try:\n",
        "          self.ftp.mkd(item)\n",
        "        except:\n",
        "          debug_print('目录已存在 %s' %item)\n",
        "        self.upload_files(src, item)\n",
        "      else:\n",
        "        # if ignore ==True and (item in ignorelist):\n",
        "        #   print ('upload pass %s' %item)\n",
        "        # else:\n",
        "        self.upload_file(src, item)\n",
        "    self.ftp.cwd('..')\n",
        "\n",
        "  def get_file_list(self, line):\n",
        "    ret_arr = []\n",
        "    file_arr = self.get_filename(line)\n",
        "    if file_arr[1] not in ['.', '..']:\n",
        "      self.file_list.append(file_arr)\n",
        "      \n",
        "  def get_filename(self, line):\n",
        "    pos = line.rfind(':')\n",
        "    while(line[pos] != ' '):\n",
        "      pos += 1\n",
        "    while(line[pos] == ' '):\n",
        "      pos += 1\n",
        "    file_arr = [line[0], line[pos:]]\n",
        "    return file_arr\n",
        "def debug_print(s):\n",
        "  print (s)\n",
        "def deal_error(e):\n",
        "  timenow  = time.localtime()\n",
        "  datenow  = time.strftime('%Y-%m-%d', timenow)\n",
        "  logstr = '%s 发生错误: %s' %(datenow, e)\n",
        "  debug_print(logstr)\n",
        "  file.write(logstr)\n",
        "  sys.exit()\n",
        "# #### download the init datasets/  \n",
        "# if __name__ == '__main__':\n",
        "#   file = open(\"log.txt\", \"a\")\n",
        "#   timenow  = time.localtime()\n",
        "#   datenow  = time.strftime('%Y-%m-%d', timenow)\n",
        "#   logstr = datenow\n",
        "#   # 配置如下变量\n",
        "#   hostaddr = '95.179.235.150' # ftp地址\n",
        "#   username = 'anonymous' # 用户名\n",
        "#   password = '' # 密码\n",
        "#   port  =  21   # 端口号 \n",
        "#   rootdir_local  = os.path.join('datasets/') # 本地目录\n",
        "#   rootdir_remote = '/anonymous/'          # 远程目录\n",
        "#   f = MYFTP(hostaddr, username, password, rootdir_remote, port)\n",
        "#   f.login()\n",
        "#   timenow  = time.localtime()\n",
        "#   datenow  = time.strftime('%Y-%m-%d-%H-%M-%s', timenow)\n",
        "#   logstr += \" - %s 登录成功\\n\" %datenow\n",
        "#   debug_print(logstr) \n",
        "# #   f.download_files(rootdir_local, rootdir_remote)\n",
        "#   # f.upload_file(rootdir_local,'sample.tgz')\n",
        "#   localdir  = rootdir_local\n",
        "#   remotedir = os.path.join(rootdir_remote,'datasets')\n",
        "#   f.download_files(localdir, remotedir)\n",
        "#   timenow  = time.localtime()\n",
        "#   datenow  = time.strftime('%Y-%m-%d-%H-%M-%s', timenow)  \n",
        "#   logstr += \" - %s 成功执行了备份\\n\" %datenow\n",
        "#   debug_print(logstr)\n",
        "  \n",
        "#   file.write(logstr)\n",
        "#   file.close()\n",
        "#   !ls -lrt datasets/\n",
        "def mydownload(localdir,remotedir=''):\n",
        "\n",
        "\n",
        "  file = open(\"log.txt\", \"a\")\n",
        "  timenow  = time.localtime()\n",
        "  datenow  = time.strftime('%Y-%m-%d', timenow)\n",
        "  logstr = datenow\n",
        "  # 配置如下变量\n",
        "  hostaddr = '95.179.235.150' # ftp地址\n",
        "  username = 'ftptest' # 用户名\n",
        "  password = 'ftp123' # 密码\n",
        "  port  =  21   # 端口号 \n",
        "  rootdir_remote = '/home/ftptest/'          # 远程目录\n",
        "  f = MYFTP(hostaddr, username, password, rootdir_remote, port)\n",
        "  f.login()\n",
        "  timenow  = time.localtime()\n",
        "  datenow  = time.strftime('%Y-%m-%d-%H-%M-%s', timenow)\n",
        "  logstr += \" - %s 登录成功\\n\" %datenow\n",
        "  debug_print(logstr) \n",
        "  if remotedir =='':\n",
        "      remotedir = localdir\n",
        "  print (\"本地目录:%s\"%localdir)\n",
        "  print (\"远程目录:%s\"%remotedir)      \n",
        "  remotedir = os.path.join(rootdir_remote,remotedir)\n",
        "  f.download_files(localdir, remotedir)\n",
        "  timenow  = time.localtime()\n",
        "  datenow  = time.strftime('%Y-%m-%d-%H-%M-%s', timenow)  \n",
        "  logstr += \" - %s 成功执行了备份\\n\" %datenow\n",
        "  debug_print(logstr)\n",
        "  \n",
        "  file.write(logstr)\n",
        "  file.close()\n",
        "\n",
        "\n",
        "def myupload(localdir,ignore =True ):\n",
        "  file = open(\"log.txt\", \"a\")\n",
        "  timenow  = time.localtime()\n",
        "  datenow  = time.strftime('%Y-%m-%d', timenow)\n",
        "  logstr = datenow\n",
        "  # 配置如下变量\n",
        "  hostaddr = '95.179.235.150' # ftp地址\n",
        "  # username = 'anonymous' # 用户名\n",
        "  # password = '/' # 密码\n",
        "  username = 'ftptest' # 用户名\n",
        "  password = 'ftp123' # 密码\n",
        "  port  =  21   # 端口号 \n",
        "  rootdir_local  = os.path.join('datasets/') # 本地目录\n",
        "#   rootdir_local  = 'flower_photos.tgz'\n",
        "  rootdir_remote = '/home/ftptest/'          # 远程目录\n",
        "  \n",
        "\n",
        "  f = MYFTP(hostaddr, username, password, rootdir_remote, port)\n",
        "  f.login()\n",
        "  timenow  = time.localtime()\n",
        "  datenow  = time.strftime('%Y-%m-%d-%H-%M-%s', timenow)\n",
        "  logstr += \" - %s 登录成功\\n\" %datenow\n",
        "  debug_print(logstr) \n",
        "  # localdir  = log_dir\n",
        "  remotedir = os.path.join(rootdir_remote,localdir)\n",
        "  print (\"localdir\",localdir)\n",
        "  print (\"remotedir\",remotedir)\n",
        "  \n",
        "  f.upload_files(localdir=localdir, remotedir = remotedir)\n",
        "  timenow  = time.localtime()\n",
        "  datenow  = time.strftime('%Y-%m-%d-%H:%M:%S', timenow)  \n",
        "  logstr += \" - %s 成功执行了备份\\n\" %datenow\n",
        "  debug_print(logstr)\n",
        "  \n",
        "  file.write(logstr)\n",
        "  file.close()\n",
        "  print (\"done\")    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6eSNUwLzQP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (log_dir)\n",
        "mydownload(datasets_dir)\n",
        "mydownload(log_dir)\n",
        "mydownload(log_eval)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fis4Sze9zNyd",
        "colab": {}
      },
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Provides utilities to preprocess images for the Inception networks.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.ops import control_flow_ops\n",
        "\n",
        "\n",
        "def apply_with_random_selector(x, func, num_cases):\n",
        "  \"\"\"Computes func(x, sel), with sel sampled from [0...num_cases-1].\n",
        "\n",
        "  Args:\n",
        "    x: input Tensor.\n",
        "    func: Python function to apply.\n",
        "    num_cases: Python int32, number of cases to sample sel from.\n",
        "\n",
        "  Returns:\n",
        "    The result of func(x, sel), where func receives the value of the\n",
        "    selector as a python integer, but sel is sampled dynamically.\n",
        "  \"\"\"\n",
        "  sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n",
        "  # Pass the real x only to one of the func calls.\n",
        "  return control_flow_ops.merge([\n",
        "      func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n",
        "      for case in range(num_cases)])[0]\n",
        "\n",
        "\n",
        "def distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n",
        "  \"\"\"Distort the color of a Tensor image.\n",
        "\n",
        "  Each color distortion is non-commutative and thus ordering of the color ops\n",
        "  matters. Ideally we would randomly permute the ordering of the color ops.\n",
        "  Rather then adding that level of complication, we select a distinct ordering\n",
        "  of color ops for each preprocessing thread.\n",
        "\n",
        "  Args:\n",
        "    image: 3-D Tensor containing single image in [0, 1].\n",
        "    color_ordering: Python int, a type of distortion (valid values: 0-3).\n",
        "    fast_mode: Avoids slower ops (random_hue and random_contrast)\n",
        "    scope: Optional scope for name_scope.\n",
        "  Returns:\n",
        "    3-D Tensor color-distorted image on range [0, 1]\n",
        "  Raises:\n",
        "    ValueError: if color_ordering not in [0, 3]\n",
        "  \"\"\"\n",
        "  with tf.name_scope(scope, 'distort_color', [image]):\n",
        "    if fast_mode:\n",
        "      if color_ordering == 0:\n",
        "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
        "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
        "      else:\n",
        "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
        "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
        "    else:\n",
        "      if color_ordering == 0:\n",
        "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
        "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
        "        image = tf.image.random_hue(image, max_delta=0.2)\n",
        "        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
        "      elif color_ordering == 1:\n",
        "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
        "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
        "        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
        "        image = tf.image.random_hue(image, max_delta=0.2)\n",
        "      elif color_ordering == 2:\n",
        "        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
        "        image = tf.image.random_hue(image, max_delta=0.2)\n",
        "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
        "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
        "      elif color_ordering == 3:\n",
        "        image = tf.image.random_hue(image, max_delta=0.2)\n",
        "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
        "        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
        "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
        "      else:\n",
        "        raise ValueError('color_ordering must be in [0, 3]')\n",
        "\n",
        "    # The random_* ops do not necessarily clamp.\n",
        "    return tf.clip_by_value(image, 0.0, 1.0)\n",
        "\n",
        "\n",
        "def distorted_bounding_box_crop(image,\n",
        "                                bbox,\n",
        "                                min_object_covered=0.1,\n",
        "                                aspect_ratio_range=(0.75, 1.33),\n",
        "                                area_range=(0.05, 1.0),\n",
        "                                max_attempts=100,\n",
        "                                scope=None):\n",
        "  \"\"\"Generates cropped_image using a one of the bboxes randomly distorted.\n",
        "\n",
        "  See `tf.image.sample_distorted_bounding_box` for more documentation.\n",
        "\n",
        "  Args:\n",
        "    image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n",
        "    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n",
        "      where each coordinate is [0, 1) and the coordinates are arranged\n",
        "      as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n",
        "      image.\n",
        "    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n",
        "      area of the image must contain at least this fraction of any bounding box\n",
        "      supplied.\n",
        "    aspect_ratio_range: An optional list of `floats`. The cropped area of the\n",
        "      image must have an aspect ratio = width / height within this range.\n",
        "    area_range: An optional list of `floats`. The cropped area of the image\n",
        "      must contain a fraction of the supplied image within in this range.\n",
        "    max_attempts: An optional `int`. Number of attempts at generating a cropped\n",
        "      region of the image of the specified constraints. After `max_attempts`\n",
        "      failures, return the entire image.\n",
        "    scope: Optional scope for name_scope.\n",
        "  Returns:\n",
        "    A tuple, a 3-D Tensor cropped_image and the distorted bbox\n",
        "  \"\"\"\n",
        "  with tf.name_scope(scope, 'distorted_bounding_box_crop', [image, bbox]):\n",
        "    # Each bounding box has shape [1, num_boxes, box coords] and\n",
        "    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n",
        "\n",
        "    # A large fraction of image datasets contain a human-annotated bounding\n",
        "    # box delineating the region of the image containing the object of interest.\n",
        "    # We choose to create a new bounding box for the object which is a randomly\n",
        "    # distorted version of the human-annotated bounding box that obeys an\n",
        "    # allowed range of aspect ratios, sizes and overlap with the human-annotated\n",
        "    # bounding box. If no box is supplied, then we assume the bounding box is\n",
        "    # the entire image.\n",
        "    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n",
        "        tf.shape(image),\n",
        "        bounding_boxes=bbox,\n",
        "        min_object_covered=min_object_covered,\n",
        "        aspect_ratio_range=aspect_ratio_range,\n",
        "        area_range=area_range,\n",
        "        max_attempts=max_attempts,\n",
        "        use_image_if_no_bounding_boxes=True)\n",
        "    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n",
        "\n",
        "    # Crop the image to the specified bounding box.\n",
        "    cropped_image = tf.slice(image, bbox_begin, bbox_size)\n",
        "    return cropped_image, distort_bbox\n",
        "\n",
        "\n",
        "def preprocess_for_train(image, height, width, bbox,\n",
        "                         fast_mode=True,\n",
        "                         scope=None):\n",
        "  \"\"\"Distort one image for training a network.\n",
        "\n",
        "  Distorting images provides a useful technique for augmenting the data\n",
        "  set during training in order to make the network invariant to aspects\n",
        "  of the image that do not effect the label.\n",
        "\n",
        "  Additionally it would create image_summaries to display the different\n",
        "  transformations applied to the image.\n",
        "\n",
        "  Args:\n",
        "    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n",
        "      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n",
        "      is [0, MAX], where MAX is largest positive representable number for\n",
        "      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n",
        "    height: integer\n",
        "    width: integer\n",
        "    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n",
        "      where each coordinate is [0, 1) and the coordinates are arranged\n",
        "      as [ymin, xmin, ymax, xmax].\n",
        "    fast_mode: Optional boolean, if True avoids slower transformations (i.e.\n",
        "      bi-cubic resizing, random_hue or random_contrast).\n",
        "    scope: Optional scope for name_scope.\n",
        "  Returns:\n",
        "    3-D float Tensor of distorted image used for training with range [-1, 1].\n",
        "  \"\"\"\n",
        "  with tf.name_scope(scope, 'distort_image', [image, height, width, bbox]):\n",
        "    if bbox is None:\n",
        "      bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n",
        "                         dtype=tf.float32,\n",
        "                         shape=[1, 1, 4])\n",
        "    if image.dtype != tf.float32:\n",
        "      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "    # Each bounding box has shape [1, num_boxes, box coords] and\n",
        "    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n",
        "    image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n",
        "                                                  bbox)\n",
        "    tf.summary.image('image_with_bounding_boxes', image_with_box)\n",
        "\n",
        "    distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)\n",
        "    # Restore the shape since the dynamic slice based upon the bbox_size loses\n",
        "    # the third dimension.\n",
        "    distorted_image.set_shape([None, None, 3])\n",
        "    image_with_distorted_box = tf.image.draw_bounding_boxes(\n",
        "        tf.expand_dims(image, 0), distorted_bbox)\n",
        "    tf.summary.image('images_with_distorted_bounding_box',\n",
        "                     image_with_distorted_box)\n",
        "\n",
        "    # This resizing operation may distort the images because the aspect\n",
        "    # ratio is not respected. We select a resize method in a round robin\n",
        "    # fashion based on the thread number.\n",
        "    # Note that ResizeMethod contains 4 enumerated resizing methods.\n",
        "\n",
        "    # We select only 1 case for fast_mode bilinear.\n",
        "    num_resize_cases = 1 if fast_mode else 4\n",
        "    distorted_image = apply_with_random_selector(\n",
        "        distorted_image,\n",
        "        lambda x, method: tf.image.resize_images(x, [height, width], method=method),\n",
        "        num_cases=num_resize_cases)\n",
        "\n",
        "    tf.summary.image('cropped_resized_image',\n",
        "                     tf.expand_dims(distorted_image, 0))\n",
        "\n",
        "    # Randomly flip the image horizontally.\n",
        "    distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
        "\n",
        "    # Randomly distort the colors. There are 4 ways to do it.\n",
        "    distorted_image = apply_with_random_selector(\n",
        "        distorted_image,\n",
        "        lambda x, ordering: distort_color(x, ordering, fast_mode),\n",
        "        num_cases=4)\n",
        "\n",
        "    tf.summary.image('final_distorted_image',\n",
        "                     tf.expand_dims(distorted_image, 0))\n",
        "    distorted_image = tf.subtract(distorted_image, 0.5)\n",
        "    distorted_image = tf.multiply(distorted_image, 2.0)\n",
        "    return distorted_image\n",
        "\n",
        "\n",
        "def preprocess_for_eval(image, height, width,\n",
        "                        central_fraction=0.875, scope=None):\n",
        "  \"\"\"Prepare one image for evaluation.\n",
        "\n",
        "  If height and width are specified it would output an image with that size by\n",
        "  applying resize_bilinear.\n",
        "\n",
        "  If central_fraction is specified it would cropt the central fraction of the\n",
        "  input image.\n",
        "\n",
        "  Args:\n",
        "    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n",
        "      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n",
        "      is [0, MAX], where MAX is largest positive representable number for\n",
        "      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details)\n",
        "    height: integer\n",
        "    width: integer\n",
        "    central_fraction: Optional Float, fraction of the image to crop.\n",
        "    scope: Optional scope for name_scope.\n",
        "  Returns:\n",
        "    3-D float Tensor of prepared image.\n",
        "  \"\"\"\n",
        "  with tf.name_scope(scope, 'eval_image', [image, height, width]):\n",
        "    if image.dtype != tf.float32:\n",
        "      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "    # Crop the central region of the image with an area containing 87.5% of\n",
        "    # the original image.\n",
        "    if central_fraction:\n",
        "      image = tf.image.central_crop(image, central_fraction=central_fraction)\n",
        "\n",
        "    if height and width:\n",
        "      # Resize the image to the specified height and width.\n",
        "      image = tf.expand_dims(image, 0)\n",
        "      image = tf.image.resize_bilinear(image, [height, width],\n",
        "                                       align_corners=False)\n",
        "      image = tf.squeeze(image, [0])\n",
        "    image = tf.subtract(image, 0.5)\n",
        "    image = tf.multiply(image, 2.0)\n",
        "    return image\n",
        "\n",
        "\n",
        "def preprocess_image(image, height, width,\n",
        "                     is_training=False,\n",
        "                     bbox=None,\n",
        "                     fast_mode=True):\n",
        "  \"\"\"Pre-process one image for training or evaluation.\n",
        "\n",
        "  Args:\n",
        "    image: 3-D Tensor [height, width, channels] with the image.\n",
        "    height: integer, image expected height.\n",
        "    width: integer, image expected width.\n",
        "    is_training: Boolean. If true it would transform an image for train,\n",
        "      otherwise it would transform it for evaluation.\n",
        "    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n",
        "      where each coordinate is [0, 1) and the coordinates are arranged as\n",
        "      [ymin, xmin, ymax, xmax].\n",
        "    fast_mode: Optional boolean, if True avoids slower transformations.\n",
        "\n",
        "  Returns:\n",
        "    3-D float Tensor containing an appropriately scaled image\n",
        "\n",
        "  Raises:\n",
        "    ValueError: if user does not provide bounding box\n",
        "  \"\"\"\n",
        "  if is_training:\n",
        "    return preprocess_for_train(image, height, width, bbox, fast_mode)\n",
        "  else:\n",
        "    return preprocess_for_eval(image, height, width)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cikXjRZ1RvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##train_flowers\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\n",
        "from tensorflow.python.platform import tf_logging as logging\n",
        "# import inception_preprocessing\n",
        "# from tensorflow.models.research.slim.preprocessing import inception_preprocessing\n",
        "from tensorflow.contrib.slim.nets import inception #inception_resnet_v2, inception_resnet_v2_arg_scope,inception_v3,inception_v3_arg_scope\n",
        "# from tensorflow.contrib.slim.python.slim.nets import inception_v2.py\n",
        "import os\n",
        "import time\n",
        "slim = tf.contrib.slim\n",
        "\n",
        "#================ DATASET INFORMATION ======================\n",
        "#State dataset directory where the tfrecord files are located\n",
        "dataset_dir = './datasets/'\n",
        "\n",
        "#State where your log file is at. If it doesn't exist, create it.\n",
        "log_dir = 'log'\n",
        "\n",
        "#State where your checkpoint file is\n",
        "inception_version =['v2','v3'][1]\n",
        "if inception_version=='v2':\n",
        "    checkpoint_file = os.path.join(dataset_dir,'inception_resnet_v2_2016_08_30.ckpt')\n",
        "elif inception_version =='v3':\n",
        "    checkpoint_file = os.path.join(dataset_dir,'inception_v3.ckpt')\n",
        "else :\n",
        "    checkpoint_file = None\n",
        "#State the image size you're resizing your images to. We will use the default inception size of 299.\n",
        "image_size = 299\n",
        "\n",
        "#State the number of classes to predict:\n",
        "num_classes = 5\n",
        "\n",
        "#State the labels file and read it\n",
        "labels_file = os.path.join('datasets/','./labels.txt')\n",
        "labels = open(labels_file, 'r')\n",
        "\n",
        "#Create a dictionary to refer each label to their string name\n",
        "labels_to_name = {}\n",
        "for line in labels:\n",
        "    label, string_name = line.split(':')\n",
        "    string_name = string_name[:-1] #Remove newline\n",
        "    labels_to_name[int(label)] = string_name\n",
        "\n",
        "#Create the file pattern of your TFRecord files so that it could be recognized later on\n",
        "file_pattern = 'flowers_%s_*.tfrecord'\n",
        "\n",
        "#Create a dictionary that will help people understand your dataset better. This is required by the Dataset class later.\n",
        "items_to_descriptions = {\n",
        "    'image': 'A 3-channel RGB coloured flower image that is either tulips, sunflowers, roses, dandelion, or daisy.',\n",
        "    'label': 'A label that is as such -- 0:daisy, 1:dandelion, 2:roses, 3:sunflowers, 4:tulips'\n",
        "}\n",
        "\n",
        "\n",
        "#================= TRAINING INFORMATION ==================\n",
        "#State the number of epochs to train\n",
        "num_epochs = num_epochs_train\n",
        "\n",
        "#State your batch size\n",
        "batch_size = 16\n",
        "\n",
        "#Learning rate information and configuration (Up to you to experiment)\n",
        "initial_learning_rate = 0.0002\n",
        "learning_rate_decay_factor = 0.7\n",
        "num_epochs_before_decay = 2\n",
        "\n",
        "upload_step =5000\n",
        "#============== DATASET LOADING ======================\n",
        "#We now create a function that creates a Dataset class which will give us many TFRecord files to feed in the examples into a queue in parallel.\n",
        "def get_split(split_name, dataset_dir, file_pattern=file_pattern, file_pattern_for_counting='flowers'):\n",
        "    '''\n",
        "    Obtains the split - training or validation - to create a Dataset class for feeding the examples into a queue later on. This function will\n",
        "    set up the decoder and dataset information all into one Dataset class so that you can avoid the brute work later on.\n",
        "    Your file_pattern is very important in locating the files later. \n",
        "\n",
        "    INPUTS:\n",
        "    - split_name(str): 'train' or 'validation'. Used to get the correct data split of tfrecord files\n",
        "    - dataset_dir(str): the dataset directory where the tfrecord files are located\n",
        "    - file_pattern(str): the file name structure of the tfrecord files in order to get the correct data\n",
        "    - file_pattern_for_counting(str): the string name to identify your tfrecord files for counting\n",
        "\n",
        "    OUTPUTS:\n",
        "    - dataset (Dataset): A Dataset class object where we can read its various components for easier batch creation later.\n",
        "    '''\n",
        "\n",
        "    #First check whether the split_name is train or validation\n",
        "    if split_name not in ['train', 'validation']:\n",
        "        raise ValueError('The split_name %s is not recognized. Please input either train or validation as the split_name' % (split_name))\n",
        "\n",
        "    #Create the full path for a general file_pattern to locate the tfrecord_files\n",
        "    file_pattern_path = os.path.join(dataset_dir, file_pattern % (split_name))\n",
        "\n",
        "    #Count the total number of examples in all of these shard\n",
        "    num_samples = 0\n",
        "    file_pattern_for_counting = file_pattern_for_counting + '_' + split_name\n",
        "    tfrecords_to_count = [os.path.join(dataset_dir, file) for file in os.listdir(dataset_dir) if file.startswith(file_pattern_for_counting)]\n",
        "    for tfrecord_file in tfrecords_to_count:\n",
        "        for record in tf.python_io.tf_record_iterator(tfrecord_file):\n",
        "            num_samples += 1\n",
        "\n",
        "    #Create a reader, which must be a TFRecord reader in this case\n",
        "    reader = tf.TFRecordReader\n",
        "\n",
        "    #Create the keys_to_features dictionary for the decoder\n",
        "    keys_to_features = {\n",
        "      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n",
        "      'image/format': tf.FixedLenFeature((), tf.string, default_value='jpg'),\n",
        "      'image/class/label': tf.FixedLenFeature(\n",
        "          [], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),\n",
        "    }\n",
        "\n",
        "    #Create the items_to_handlers dictionary for the decoder.\n",
        "    items_to_handlers = {\n",
        "    'image': slim.tfexample_decoder.Image(),\n",
        "    'label': slim.tfexample_decoder.Tensor('image/class/label'),\n",
        "    }\n",
        "\n",
        "    #Start to create the decoder\n",
        "    decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features, items_to_handlers)\n",
        "\n",
        "    #Create the labels_to_name file\n",
        "    labels_to_name_dict = labels_to_name\n",
        "\n",
        "    #Actually create the dataset\n",
        "    dataset = slim.dataset.Dataset(\n",
        "        data_sources = file_pattern_path,\n",
        "        decoder = decoder,\n",
        "        reader = reader,\n",
        "        num_readers = 4,\n",
        "        num_samples = num_samples,\n",
        "        num_classes = num_classes,\n",
        "        labels_to_name = labels_to_name_dict,\n",
        "        items_to_descriptions = items_to_descriptions)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def load_batch(dataset, batch_size, height=image_size, width=image_size, is_training=True):\n",
        "    '''\n",
        "    Loads a batch for training.\n",
        "\n",
        "    INPUTS:\n",
        "    - dataset(Dataset): a Dataset class object that is created from the get_split function\n",
        "    - batch_size(int): determines how big of a batch to train\n",
        "    - height(int): the height of the image to resize to during preprocessing\n",
        "    - width(int): the width of the image to resize to during preprocessing\n",
        "    - is_training(bool): to determine whether to perform a training or evaluation preprocessing\n",
        "\n",
        "    OUTPUTS:\n",
        "    - images(Tensor): a Tensor of the shape (batch_size, height, width, channels) that contain one batch of images\n",
        "    - labels(Tensor): the batch's labels with the shape (batch_size,) (requires one_hot_encoding).\n",
        "\n",
        "    '''\n",
        "    #First create the data_provider object\n",
        "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
        "        dataset,\n",
        "        common_queue_capacity = 24 + 3 * batch_size,\n",
        "        common_queue_min = 24)\n",
        "\n",
        "    #Obtain the raw image using the get method\n",
        "    raw_image, label = data_provider.get(['image', 'label'])\n",
        "\n",
        "    #Perform the correct preprocessing for this image depending if it is training or evaluating\n",
        "#     image = inception_preprocessing.preprocess_image(raw_image, height, width, is_training)\n",
        "    image = preprocess_image(raw_image, height, width, is_training)\n",
        "    #As for the raw images, we just do a simple reshape to batch it up\n",
        "    raw_image = tf.expand_dims(raw_image, 0)\n",
        "    raw_image = tf.image.resize_nearest_neighbor(raw_image, [height, width])\n",
        "    raw_image = tf.squeeze(raw_image)\n",
        "\n",
        "    #Batch up the image by enqueing the tensors internally in a FIFO queue and dequeueing many elements with tf.train.batch.\n",
        "    images, raw_images, labels = tf.train.batch(\n",
        "        [image, raw_image, label],\n",
        "        batch_size = batch_size,\n",
        "        num_threads = 4,\n",
        "        capacity = 4 * batch_size,\n",
        "        allow_smaller_final_batch = True)\n",
        "\n",
        "    return images, raw_images, labels\n",
        "\n",
        "def run():\n",
        "    #Create the log directory here. Must be done here otherwise import will activate this unneededly.\n",
        "    if not os.path.exists(log_dir):\n",
        "        os.mkdir(log_dir)\n",
        "\n",
        "    #======================= TRAINING PROCESS =========================\n",
        "    #Now we start to construct the graph and build our model\n",
        "    with tf.Graph().as_default() as graph:\n",
        "        tf.logging.set_verbosity(tf.logging.INFO) #Set the verbosity to INFO level\n",
        "\n",
        "        #First create the dataset and load one batch\n",
        "        dataset = get_split('train', dataset_dir, file_pattern=file_pattern)\n",
        "        images, _, labels = load_batch(dataset, batch_size=batch_size)\n",
        "\n",
        "        #Know the number steps to take before decaying the learning rate and batches per epoch\n",
        "        num_batches_per_epoch = int(dataset.num_samples / batch_size)\n",
        "        num_steps_per_epoch = num_batches_per_epoch #Because one step is one batch processed\n",
        "        decay_steps = int(num_epochs_before_decay * num_steps_per_epoch)\n",
        "        if inception_version =='v2':\n",
        "            #Create the model inference\n",
        "            with slim.arg_scope(inception.inception_resnet_v2_arg_scope()):\n",
        "                logits, end_points = inception.inception_resnet_v2(images, num_classes = dataset.num_classes, is_training = True)\n",
        "\n",
        "            #Define the scopes that you want to exclude for restoration\n",
        "            exclude = ['InceptionResnetV2/Logits', 'InceptionResnetV2/AuxLogits']\n",
        "            variables_to_restore = slim.get_variables_to_restore(exclude = exclude)\n",
        "        elif inception_version =='v3':\n",
        "            with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
        "                logits, end_points = inception.inception_v3(images, num_classes = dataset.num_classes, is_training = True)\n",
        "\n",
        "            #Define the scopes that you want to exclude for restoration\n",
        "            exclude = ['InceptionV3/Logits', 'InceptionV3/AuxLogits']\n",
        "            variables_to_restore = slim.get_variables_to_restore(exclude = exclude)            \n",
        "        #Perform one-hot-encoding of the labels (Try one-hot-encoding within the load_batch function!)\n",
        "        one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\n",
        "\n",
        "        #Performs the equivalent to tf.nn.sparse_softmax_cross_entropy_with_logits but enhanced with checks\n",
        "        loss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits = logits)\n",
        "        total_loss = tf.losses.get_total_loss()    #obtain the regularization losses as well\n",
        "\n",
        "        #Create the global step for monitoring the learning_rate and training.\n",
        "        global_step = get_or_create_global_step()\n",
        "        mprint(\"global_step\",global_step)\n",
        "        #Define your exponentially decaying learning rate\n",
        "        lr = tf.train.exponential_decay(\n",
        "            learning_rate = initial_learning_rate,\n",
        "            global_step = global_step,\n",
        "            decay_steps = decay_steps,\n",
        "            decay_rate = learning_rate_decay_factor,\n",
        "            staircase = True)\n",
        "\n",
        "        #Now we can define the optimizer that takes on the learning rate\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate = lr)\n",
        "\n",
        "        #Create the train_op.\n",
        "        train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
        "\n",
        "        #State the metrics that you want to predict. We get a predictions that is not one_hot_encoded.\n",
        "        predictions = tf.argmax(end_points['Predictions'], 1)\n",
        "        probabilities = end_points['Predictions']\n",
        "        accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)\n",
        "        metrics_op = tf.group(accuracy_update, probabilities)\n",
        "\n",
        "\n",
        "        #Now finally create all the summaries you need to monitor and group them into one summary op.\n",
        "        tf.summary.scalar('losses/Total_Loss', total_loss)\n",
        "        tf.summary.scalar('accuracy', accuracy)\n",
        "        tf.summary.scalar('learning_rate', lr)\n",
        "        my_summary_op = tf.summary.merge_all()\n",
        "\n",
        "        #Now we need to create a training step function that runs both the train_op, metrics_op and updates the global_step concurrently.\n",
        "        def train_step(sess, train_op, global_step):\n",
        "            '''\n",
        "            Simply runs a session for the three arguments provided and gives a logging on the time elapsed for each global step\n",
        "            '''\n",
        "            #Check the time for each sess run\n",
        "            start_time = time.time()\n",
        "            total_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])\n",
        "            time_elapsed = time.time() - start_time\n",
        "\n",
        "            #Run the logging to print some results\n",
        "            logging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, time_elapsed)\n",
        "            mprint('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, time_elapsed) \n",
        "            return total_loss, global_step_count\n",
        "\n",
        "        #Now we create a saver function that actually restores the variables from a checkpoint file in a sess\n",
        "        saver = tf.train.Saver(variables_to_restore)\n",
        "        def restore_fn(sess):\n",
        "            return saver.restore(sess, checkpoint_file)\n",
        "\n",
        "        #Define your supervisor for running a managed session. Do not run the summary_op automatically or else it will consume too much memory\n",
        "        sv = tf.train.Supervisor(logdir = log_dir, summary_op = None, init_fn = restore_fn)\n",
        "\n",
        "\n",
        "        #Run the managed session\n",
        "        with sv.managed_session() as sess:\n",
        "            for step in range(num_steps_per_epoch * num_epochs):\n",
        "                #At the start of every epoch, show the vital information:\n",
        "                if step % num_batches_per_epoch == 0:\n",
        "                    logging.info('Epoch %s/%s', step/num_batches_per_epoch + 1, num_epochs)\n",
        "                    \n",
        "                    learning_rate_value, accuracy_value = sess.run([lr, accuracy])\n",
        "                    logging.info('Current Learning Rate: %s', learning_rate_value)\n",
        "                    logging.info('Current Streaming Accuracy: %s', accuracy_value)\n",
        "\n",
        "                    mprint('Epoch %s/%s', step/num_batches_per_epoch + 1, num_epochs)\n",
        "                    mprint('Current Learning Rate: %s', learning_rate_value)\n",
        "                    mprint('Current Streaming Accuracy: %s', accuracy_value)\n",
        "                    # optionally, print your logits and predictions for a sanity check that things are going fine.\n",
        "                    logits_value, probabilities_value, predictions_value, labels_value = sess.run([logits, probabilities, predictions, labels])\n",
        "                    mprint ('logits: \\n', logits_value                 )\n",
        "                    mprint ('Probabilities: \\n', probabilities_value   )\n",
        "                    mprint ('predictions: \\n', predictions_value       )\n",
        "                    mprint ('Labels:\\n:', labels_value                 )\n",
        "\n",
        "                #Log the summaries every 10 step.\n",
        "                if step % 10 == 0:\n",
        "                    loss, _ = train_step(sess, train_op, sv.global_step)\n",
        "                    summaries = sess.run(my_summary_op)\n",
        "                    sv.summary_computed(sess, summaries)\n",
        "\n",
        "                if (step+1) %(upload_step)  ==0:    \n",
        "                    try:  \n",
        "                        myupload(log_dir)                              \n",
        "                    except Exception,info:\n",
        "                        mail('训练%d次上传文件失败:%s'%(upload_step,str(info)))\n",
        "                #If not, simply run the training step\n",
        "                else:\n",
        "                    loss, _ = train_step(sess, train_op, sv.global_step)\n",
        "\n",
        "            #We log the final training loss and accuracy\n",
        "            logging.info('Final Loss: %s', loss)\n",
        "            logging.info('Final Accuracy: %s', sess.run(accuracy))\n",
        "            #Once all the training has been done, save the log files and checkpoint model\n",
        "            logging.info('Finished training! Saving model to disk now.')\n",
        "            mprint('Final Loss: %s', loss)            \n",
        "            mprint('Final Accuracy: %s', sess.run(accuracy))\n",
        "            mprint('Finished training! Saving model to disk now.')\n",
        "            # saver.save(sess, \"./flowers_model.ckpt\")\n",
        "            mail('训练 %d周期完毕 Final Loss: %s Final Accuracy: %s' % (num_epochs,loss,sess.run(accuracy)) )\n",
        "            sv.saver.save(sess, sv.save_path, global_step = sv.global_step)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run()\n",
        "    pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mn7va8IFE7Lt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##eval_flowers\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.platform import tf_logging as logging\n",
        "from tensorflow.contrib.framework.python.ops.variables import get_or_create_global_step\n",
        "# import inception_preprocessing\n",
        "from tensorflow.contrib.slim.nets import inception #inception_resnet_v2, inception_resnet_v2_arg_scope,inception_v3,inception_v3_arg_scope\n",
        "import time\n",
        "import os\n",
        "#from train_flowers import get_split, load_batch\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "slim = tf.contrib.slim\n",
        "\n",
        "\n",
        "if not os.path.isdir(log_eval):\n",
        "    os.makedirs(log_eval)\n",
        "#State the dataset directory where the validation set is found\n",
        "dataset_dir = './datasets/'\n",
        "\n",
        "\n",
        "#State the batch_size to evaluate each time, which can be a lot more than the training batch\n",
        "batch_size = 36\n",
        "\n",
        "#State the number of epochs to evaluate\n",
        "num_epochs = num_epochs_eval\n",
        "\n",
        "#Get the latest checkpoint file\n",
        "checkpoint_file = tf.train.latest_checkpoint(log_dir)\n",
        "\n",
        "\n",
        "##\n",
        "def run():\n",
        "    #Create log_dir for evaluation information\n",
        "    if not os.path.exists(log_eval):\n",
        "        os.mkdir(log_eval)\n",
        "\n",
        "    #Just construct the graph from scratch again\n",
        "    with tf.Graph().as_default() as graph:\n",
        "        tf.logging.set_verbosity(tf.logging.INFO)\n",
        "        #Get the dataset first and load one batch of validation images and labels tensors. Set is_training as False so as to use the evaluation preprocessing\n",
        "        dataset = get_split('validation', dataset_dir)\n",
        "        images, raw_images, labels = load_batch(dataset, batch_size = batch_size, is_training = False)\n",
        "\n",
        "        #Create some information about the training steps\n",
        "        num_batches_per_epoch = dataset.num_samples / batch_size\n",
        "        num_steps_per_epoch = num_batches_per_epoch\n",
        "        if inception_version =='v2':\n",
        "            #Now create the inference model but set is_training=False\n",
        "            with slim.arg_scope(inception.inception_resnet_v2_arg_scope()):\n",
        "                logits, end_points = inception.inception_resnet_v2(images, num_classes = dataset.num_classes, is_training = False)\n",
        "        elif inception_version =='v3':      \n",
        "            with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
        "                logits, end_points = inception.inception_v3(images, num_classes = dataset.num_classes, is_training = False)                \n",
        "        # #get all the variables to restore from the checkpoint file and create the saver function to restore\n",
        "        variables_to_restore = slim.get_variables_to_restore()\n",
        "        saver = tf.train.Saver(variables_to_restore)\n",
        "        def restore_fn(sess):\n",
        "            return saver.restore(sess, checkpoint_file)\n",
        "\n",
        "        #Just define the metrics to track without the loss or whatsoever\n",
        "        predictions = tf.argmax(end_points['Predictions'], 1)\n",
        "        accuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)\n",
        "        metrics_op = tf.group(accuracy_update)\n",
        "\n",
        "        #Create the global step and an increment op for monitoring\n",
        "        global_step = get_or_create_global_step()\n",
        "        global_step_op = tf.assign(global_step, global_step + 1) #no apply_gradient method so manually increasing the global_step\n",
        "        \n",
        "\n",
        "        #Create a evaluation step function\n",
        "        def eval_step(sess, metrics_op, global_step):\n",
        "            '''\n",
        "            Simply takes in a session, runs the metrics op and some logging information.\n",
        "            '''\n",
        "            start_time = time.time()\n",
        "            _, global_step_count, accuracy_value = sess.run([metrics_op, global_step_op, accuracy])\n",
        "            time_elapsed = time.time() - start_time\n",
        "\n",
        "            #Log some information\n",
        "            logging.info('Global Step %s: Streaming Accuracy: %.4f (%.2f sec/step)', global_step_count, accuracy_value, time_elapsed)\n",
        "\n",
        "            return accuracy_value\n",
        "\n",
        "\n",
        "        #Define some scalar quantities to monitor\n",
        "        tf.summary.scalar('Validation_Accuracy', accuracy)\n",
        "        my_summary_op = tf.summary.merge_all()\n",
        "\n",
        "        #Get your supervisor\n",
        "        sv = tf.train.Supervisor(logdir = log_eval, summary_op = None, saver = None, init_fn = restore_fn)\n",
        "\n",
        "        #Now we are ready to run in one session\n",
        "        with sv.managed_session() as sess:\n",
        "            total_steps = int (num_steps_per_epoch * num_epochs)\n",
        "            for step in range(total_steps):\n",
        "                sess.run(sv.global_step)\n",
        "                #print vital information every start of the epoch as always\n",
        "                if step % num_batches_per_epoch == 0:\n",
        "                    logging.info('Epoch: %s/%s', step / num_batches_per_epoch + 1, num_epochs)\n",
        "                    logging.info('Current Streaming Accuracy: %.4f', sess.run(accuracy))\n",
        "                    mprint('Epoch: %s/%s', step / num_batches_per_epoch + 1, num_epochs)\n",
        "                    mprint('Current Streaming Accuracy: %.4f', sess.run(accuracy))\n",
        "                    \n",
        "                #Compute summaries every 10 steps and continue evaluating\n",
        "                if step % 10 == 0:\n",
        "                    eval_step(sess, metrics_op = metrics_op, global_step = sv.global_step)\n",
        "                    summaries = sess.run(my_summary_op)\n",
        "                    sv.summary_computed(sess, summaries)\n",
        "                #Otherwise just run as per normal\n",
        "                else:\n",
        "                    eval_step(sess, metrics_op = metrics_op, global_step = sv.global_step)\n",
        "\n",
        "            #At the end of all the evaluation, show the final accuracy\n",
        "            logging.info('Final Streaming Accuracy: %.4f', sess.run(accuracy))\n",
        "            mprint('Final Streaming Accuracy: %.4f', sess.run(accuracy))\n",
        "            #Now we want to visualize the last batch's images just to see what our model has predicted\n",
        "            raw_images, labels, predictions = sess.run([raw_images, labels, predictions])\n",
        "            for i in range(10):\n",
        "                image, label, prediction = raw_images[i], labels[i], predictions[i]\n",
        "                prediction_name, label_name = dataset.labels_to_name[prediction], dataset.labels_to_name[label]\n",
        "                text = 'Prediction: %s \\n Ground Truth: %s' %(prediction_name, label_name)\n",
        "                img_plot = plt.imshow(image)\n",
        "\n",
        "                #Set up the plot and hide axes\n",
        "                plt.title(text)\n",
        "                img_plot.axes.get_yaxis().set_ticks([])\n",
        "                img_plot.axes.get_xaxis().set_ticks([])\n",
        "                plt.show()\n",
        "\n",
        "            logging.info('Model evaluation has completed! Visit TensorBoard for more information regarding your evaluation.')\n",
        "            mail(\"验证集测试完毕\")\n",
        "if __name__ == '__main__':\n",
        "    run()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI7Zou45UP4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "write_size =0\n",
        "mprint('log end')\n",
        "myupload(log_dir)\n",
        "myupload(log_eval)\n",
        "mail('日志文件上传完毕')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}